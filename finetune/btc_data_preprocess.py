import os
import pickle
import numpy as np
import pandas as pd
import requests
from datetime import datetime, timezone
from tqdm import trange, tqdm
import time
import random

from config import Config


class BTCDataPreprocessor:
    """
    A class to handle the loading, processing, and splitting of BTC financial data from Binance.
    """

    def __init__(self):
        """Initializes the preprocessor with configuration."""
        self.config = Config()
        self.data = {}  # A dictionary to store processed data for each symbol.
        # A list of official Binance API endpoints for redundancy.
        self.api_endpoints = [
                                 "https://api.binance.com",
                                 # "https://api1.binance.com",
                                 # "https://api2.binance.com",
                                 # "https://api3.binance.com",
                             ] * 100000

    def fetch_binance_data(self, symbol='BTCUSDT', interval='1h', start_time=None, end_time=None):
        """
        ä»Binance APIè·å–å†å²Kçº¿æ•°æ®

        Args:
            symbol (str): äº¤æ˜“å¯¹ç¬¦å·
            interval (str): Kçº¿é—´éš” ('1h', '1d', etc.)
            start_time (str): å¼€å§‹æ—¶é—´ 'YYYY-MM-DD'
            end_time (str): ç»“æŸæ—¶é—´ 'YYYY-MM-DD'

        Returns:
            pd.DataFrame: OHLCVæ•°æ®
        """
        print(f"Fetching {symbol} data from Binance...")

        # è½¬æ¢æ—¶é—´æ ¼å¼
        if start_time:
            start_ts = int(pd.Timestamp(start_time).timestamp() * 1000)
        else:
            start_ts = None

        if end_time:
            end_ts = int(pd.Timestamp(end_time).timestamp() * 1000)
        else:
            end_ts = None

        all_data = []
        current_start = start_ts
        limit = 1000  # Binance APIé™åˆ¶æ¯æ¬¡æœ€å¤š1000æ¡

        while True:
            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                'symbol': symbol,
                'interval': interval,
                'limit': limit
            }

            if current_start:
                params['startTime'] = current_start
            if end_ts:
                params['endTime'] = end_ts

            data = None
            for base_url in self.api_endpoints:
                endpoint_url = f"{base_url}/api/v3/klines"
                try:
                    # å‘é€è¯·æ±‚ï¼Œå¢åŠ 10ç§’è¶…æ—¶
                    print(f"Attempting to fetch data from {endpoint_url}...")
                    response = requests.get(endpoint_url, params=params, timeout=10)
                    response.raise_for_status()
                    data = response.json()
                    print(f"Successfully fetched a chunk from {endpoint_url}.")
                    # è¯·æ±‚æˆåŠŸï¼Œè·³å‡ºå†…éƒ¨çš„forå¾ªç¯
                    break
                except requests.exceptions.RequestException as e:
                    print(f"Failed to fetch from {endpoint_url}: {e}")
                    if base_url != self.api_endpoints[-1]:
                        print("Trying next endpoint...")

                    time.sleep(random.randint(1, 3))  # ç­‰å¾…1ç§’å†è¯•ä¸‹ä¸€ä¸ªèŠ‚ç‚¹

            # å¦‚æœå°è¯•å®Œæ‰€æœ‰èŠ‚ç‚¹ï¼Œdataä»ç„¶æ˜¯Noneï¼Œè¯´æ˜æœ¬æ¬¡è¯·æ±‚å¤±è´¥ï¼Œæ— æ³•ç»§ç»­
            if data is None:
                print("All API endpoints failed for the current request. Aborting.")
                break  # ä¸­æ–­è·å–æ•°æ®çš„whileå¾ªç¯

            if not data:
                break

            all_data.extend(data)
            print(f"Fetched {len(data)} records, total: {len(all_data)}")

            # æ›´æ–°èµ·å§‹æ—¶é—´ä¸ºæœ€åä¸€æ¡è®°å½•çš„æ—¶é—´+1
            if len(data) < limit:
                break

            current_start = data[-1][0] + 1  # æœ€åä¸€æ¡è®°å½•çš„æ—¶é—´æˆ³+1æ¯«ç§’

            # å¦‚æœå·²ç»è¶…è¿‡ç»“æŸæ—¶é—´ï¼Œåˆ™åœæ­¢
            if end_ts and current_start >= end_ts:
                break

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(0.1)

        if not all_data:
            raise ValueError("No data retrieved from Binance API")

        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(all_data, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_asset_volume', 'number_of_trades',
            'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
        ])

        # é€‰æ‹©éœ€è¦çš„åˆ—å¹¶è½¬æ¢æ•°æ®ç±»å‹
        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']].copy()
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')

        # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
        for col in ['open', 'high', 'low', 'close', 'volume']:
            df[col] = pd.to_numeric(df[col], errors='coerce')

        # å»é™¤é‡å¤å’Œç¼ºå¤±å€¼
        df = df.drop_duplicates(subset=['timestamp']).dropna()
        df = df.sort_values('timestamp').reset_index(drop=True)

        print(f"Successfully fetched {len(df)} records from {df['timestamp'].min()} to {df['timestamp'].max()}")
        return df

    def load_btc_data(self, interval='1h'):
        """
        Loads BTC data from Binance API and processes it.
        """
        print("Loading and processing BTC data from Binance...")

        # è·å–BTCæ•°æ®
        try:
            symbol_df = self.fetch_binance_data(
                symbol=self.config.symbol,
                interval=interval,
                start_time=self.config.dataset_begin_time,
                end_time=self.config.dataset_end_time
            )
        except Exception as e:
            print(f"Error fetching BTC data: {e}")
            print("Trying to load from local backup if available...")
            backup_file = 'btc_backup_data.csv'
            if os.path.exists(backup_file):
                symbol_df = pd.read_csv(backup_file)
                symbol_df['timestamp'] = pd.to_datetime(symbol_df['timestamp'])
                print(f"Loaded backup data with {len(symbol_df)} records")
            else:
                raise ValueError("Could not fetch BTC data and no backup available")

        # è®¾ç½®datetimeä¸ºç´¢å¼•
        symbol_df = symbol_df.set_index('timestamp')

        # é‡å‘½ååˆ—ä»¥åŒ¹é…åŸå§‹æ¡†æ¶æ ¼å¼
        column_mapping = {
            'volume': 'vol'
        }
        symbol_df = symbol_df.rename(columns=column_mapping)

        # æ·»åŠ amountç‰¹å¾ï¼ˆæˆäº¤é¢ = OHLCå¹³å‡ä»· Ã— æˆäº¤é‡ï¼Œä¸qlibæ–¹æ³•ä¿æŒä¸€è‡´ï¼‰
        symbol_df['amt'] = ((symbol_df['open'] + symbol_df['high'] +
                                symbol_df['low'] + symbol_df['close']) / 4) * symbol_df['vol']
        print(f"Added amount feature (OHLC average * volume) to match qlib data")

        # æ·»åŠ æ—¶é—´ç‰¹å¾ï¼ˆåŸºäºæ—¶é—´æˆ³ç´¢å¼•ï¼‰
        print("Adding time features...")
        symbol_df['minute'] = symbol_df.index.minute
        symbol_df['hour'] = symbol_df.index.hour
        symbol_df['weekday'] = symbol_df.index.weekday  # 0=Monday, 6=Sunday
        symbol_df['day'] = symbol_df.index.day
        symbol_df['month'] = symbol_df.index.month
        symbol_df['year'] = symbol_df.index.year
        print(f"Added time features: {self.config.time_feature_list}")

        # é€‰æ‹©æœ€ç»ˆç‰¹å¾ï¼ˆä»·æ ¼ç‰¹å¾ + æ—¶é—´ç‰¹å¾ï¼‰
        final_features = self.config.feature_list + self.config.time_feature_list
        symbol_df = symbol_df[final_features]

        # è¿‡æ»¤æ‰æ•°æ®ä¸è¶³çš„éƒ¨åˆ†
        symbol_df = symbol_df.dropna()

        # ä¿å­˜å¤‡ä»½
        backup_file = 'data/btc_backup_data.csv'
        os.makedirs(os.path.dirname(backup_file), exist_ok=True)  # æ·»åŠ è¿™ä¸€è¡Œ
        symbol_df.reset_index().to_csv(backup_file, index=False)
        print(f"Saved backup data to {backup_file}")

        # å­˜å‚¨å¤„ç†åçš„æ•°æ®
        self.data[self.config.symbol] = symbol_df
        print(f"Processed BTC data: {len(symbol_df)} records from {symbol_df.index.min()} to {symbol_df.index.max()}")

    def prepare_dataset(self):
        """
        Splits the loaded data into train, validation, and test sets and saves them to disk.
        """
        print("Splitting BTC data into train, validation, and test sets...")
        train_data, val_data, test_data = {}, {}, {}

        symbol = self.config.symbol
        symbol_df = self.data[symbol]

        # Define time ranges from config.
        train_start, train_end = self.config.train_time_range
        val_start, val_end = self.config.val_time_range
        test_start, test_end = self.config.test_time_range

        # Create boolean masks for each dataset split.
        train_mask = (symbol_df.index >= train_start) & (symbol_df.index <= train_end)
        val_mask = (symbol_df.index >= val_start) & (symbol_df.index <= val_end)
        test_mask = (symbol_df.index >= test_start) & (symbol_df.index <= test_end)

        # Apply masks to create the final datasets.
        train_data[symbol] = symbol_df[train_mask]
        val_data[symbol] = symbol_df[val_mask]
        test_data[symbol] = symbol_df[test_mask]

        # æ‰“å°æ•°æ®é›†ä¿¡æ¯
        print(f"Dataset split summary:")
        print(f"  Train: {len(train_data[symbol])} records ({train_start} to {train_end})")
        print(f"  Val:   {len(val_data[symbol])} records ({val_start} to {val_end})")
        print(f"  Test:  {len(test_data[symbol])} records ({test_start} to {test_end})")

        # Save the datasets using pickle.
        os.makedirs(self.config.dataset_path, exist_ok=True)

        with open(f"{self.config.dataset_path}/train_data.pkl", 'wb') as f:
            pickle.dump(train_data, f)
        with open(f"{self.config.dataset_path}/val_data.pkl", 'wb') as f:
            pickle.dump(val_data, f)
        with open(f"{self.config.dataset_path}/test_data.pkl", 'wb') as f:
            pickle.dump(test_data, f)

        print("BTC datasets prepared and saved successfully.")
        print(f"Files saved in: {self.config.dataset_path}")

    def verify_data(self):
        """éªŒè¯ç”Ÿæˆçš„æ•°æ®é›†"""
        print("\nVerifying generated datasets...")

        for split in ['train', 'val', 'test']:
            file_path = f"{self.config.dataset_path}/{split}_data.pkl"
            if os.path.exists(file_path):
                with open(file_path, 'rb') as f:
                    data = pickle.load(f)

                symbol = self.config.symbol
                if symbol in data:
                    df = data[symbol]
                    print(f"{split.upper()} dataset:")
                    print(f"  Shape: {df.shape}")
                    print(f"  Time range: {df.index.min()} to {df.index.max()}")
                    print(f"  Features: {list(df.columns)}")
                    print(f"  Sample data:")
                    print(f"    {df.head(2)}")
                    print()
            else:
                print(f"Warning: {file_path} not found")


if __name__ == '__main__':
    # This block allows the script to be run directly to perform data preprocessing.
    print("ğŸš€ Starting BTC data preprocessing...")

    preprocessor = BTCDataPreprocessor()

    # æ‰“å°é…ç½®ä¿¡æ¯
    #preprocessor.config.print_config_summary()

    try:
        # åŠ è½½å’Œå¤„ç†BTCæ•°æ®
        preprocessor.load_btc_data(interval='15m')

        # å‡†å¤‡è®­ç»ƒæ•°æ®é›†
        preprocessor.prepare_dataset()

        # éªŒè¯æ•°æ®
        preprocessor.verify_data()

        print("âœ… BTC data preprocessing completed successfully!")

    except Exception as e:
        print(f"âŒ Error during preprocessing: {e}")
        raise
